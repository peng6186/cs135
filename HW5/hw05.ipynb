{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Name and ID\n",
    "\n",
    "Pengcheng Xu  (pxu02)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW05 Code\n",
    "\n",
    "\n",
    "You will complete the following notebook, as described in the PDF for Homework 05 (included in the download with the starter code).  You will submit:\n",
    "1. This notebook file, along with your COLLABORATORS.txt file and the two tree images (PDFs generated using `graphviz` within the code), to the Gradescope link for code.\n",
    "2. A PDF of this notebook and all of its output, once it is completed, to the Gradescope link for the PDF.\n",
    "\n",
    "\n",
    "Please report any questions to the [class Piazza page](https://piazza.com/tufts/spring2021/comp135).\n",
    "\n",
    "### Import required libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import sklearn.tree\n",
    "import graphviz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Trees\n",
    "\n",
    "You should start by computing the two heuristic values for the toy data described in the assignment handout. You should then load the two versions of the abalone data, compute the two heuristic values on features (for the simplified data), and then build decision trees for each set of data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1 Compute both heuristics for toy data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (a) Compute the counting-based heuristic, and order the features by it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A: 0.75\n",
      "B: 0.75\n"
     ]
    }
   ],
   "source": [
    "def counting_based_for_A():\n",
    "    correct_count = 2 + 4 # left (i.e. o) + right (i.e. x) corrects in each sub-tree\n",
    "    total_count = 8\n",
    "    return correct_count / total_count\n",
    "\n",
    "def counting_based_for_B():\n",
    "    correct_count = 3 + 3 # eft (i.e. o) + right (i.e. x) corrects in each sub-tree\n",
    "    total_count = 8\n",
    "    return correct_count / total_count\n",
    "\n",
    "res_map = {}\n",
    "res_map['A'] = counting_based_for_A()\n",
    "res_map['B'] = counting_based_for_B()\n",
    "# sorted the result from best to worst based on accurate value\n",
    "res_map = sorted(res_map.items(), key=lambda x: x[1], reverse=True)   \n",
    "# print out the result\n",
    "for f_name, f_value in res_map:\n",
    "    print(f'{f_name}: {f_value}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (b) Compute the information-theoretic heuristic, and order the features by it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "B: 1.811\n",
      "A: 1.689\n"
     ]
    }
   ],
   "source": [
    "import math \n",
    "\n",
    "entropy_example = -(4/8 * math.log(4/8, 2) + 4/8 * math.log(4/8, 2))\n",
    "\n",
    "entropy_remainder_A = -( 2 / 8 * 0 + 6 / 8 * ( -(2/6*math.log(2/6, 2) + 4/6*math.log(4/6, 2)) ) )\n",
    "\n",
    "gain_A = entropy_example - entropy_remainder_A \n",
    "\n",
    "entropy_remainder_B = -(4/8 * -(3/4 * math.log(3/4,2) + 1/4*math.log(1/4,2)) + 4/8 * -(1/4*math.log(1/4,2) + 3/4*math.log(3/4, 2)))\n",
    "\n",
    "gain_B = entropy_example - entropy_remainder_B\n",
    "\n",
    "res_info = {}\n",
    "res_info[\"A\"] = gain_A\n",
    "res_info[\"B\"] = gain_B\n",
    "res_info = sorted(res_info.items(), key=lambda item: item[1], reverse = True)\n",
    "for f_name, f_value in res_info:\n",
    "    print(f'{f_name}: {f_value:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (c) Discussion of results.\n",
    "\n",
    "> This means when we use counting-based heuristic, we're gonna use either feature A or B as our root node, since the couting-base criterioin is the same, we cannot differentiate between A and B.\n",
    ">\n",
    "> When we use information-based heuristic, we're gonna use B as our root classification node, since we would gain more infomation by using B compared to A.\n",
    ">\n",
    "> This shows the information-based heuristic is preferable, since it can help us choose a better one when counting-based heuristic is stuck."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 Compute both heuristics for simplified abalone data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (a) Compute the counting-based heuristic, and order the features by it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "height_mm: 0.7292191435768262\n",
      "diam_mm: 0.7134760705289672\n",
      "length_mm: 0.7021410579345088\n",
      "is_male: 0.5869017632241813\n"
     ]
    }
   ],
   "source": [
    "simplified_x_train = pd.read_csv(\"data_abalone/small_binary_x_train.csv\")\n",
    "simplified_y_train = pd.read_csv(\"data_abalone/3class_y_train.csv\")\n",
    "\n",
    "num_data = len(simplified_y_train)\n",
    "# print(\"# of data: \", num_data)\n",
    "\n",
    "# couting_based heuristic for is_male\n",
    "y_group_ismale_0 = simplified_y_train[simplified_x_train['is_male'] == 0]\n",
    "y_group_ismale_1 = simplified_y_train[simplified_x_train['is_male'] == 1]\n",
    "\n",
    "y_group_ismale_0_output_label = y_group_ismale_0.value_counts().index[0][0]\n",
    "y_group_ismale_1_output_label = y_group_ismale_1.value_counts().index[0][0]\n",
    "\n",
    "y_group_ismale_0_correct = np.sum(y_group_ismale_0 == y_group_ismale_0_output_label)\n",
    "y_group_ismale_1_correct = np.sum(y_group_ismale_1 == y_group_ismale_1_output_label)\n",
    "\n",
    "correct_rate_ismale = (y_group_ismale_0_correct + y_group_ismale_1_correct) / num_data\n",
    "\n",
    "# couting_based heuristic for length_mm\n",
    "y_group_lengthmm_0 = simplified_y_train[simplified_x_train['length_mm'] == 0]\n",
    "y_group_lengthmm_1 = simplified_y_train[simplified_x_train['length_mm'] == 1]\n",
    "\n",
    "y_group_lengthmm_0_output_label = y_group_lengthmm_0.value_counts().index[0][0]\n",
    "y_group_lengthmm_1_output_label = y_group_lengthmm_1.value_counts().index[0][0]\n",
    "\n",
    "y_group_lengthmm_0_correct = np.sum(y_group_lengthmm_0 == y_group_lengthmm_0_output_label)\n",
    "y_group_lengthmm_1_correct = np.sum(y_group_lengthmm_1 == y_group_lengthmm_1_output_label)\n",
    "\n",
    "correct_rate_lengthmm = (y_group_lengthmm_0_correct + y_group_lengthmm_1_correct) / num_data\n",
    "# couting_based heuristic for diam_mm\n",
    "y_group_diammm_0 = simplified_y_train[simplified_x_train['diam_mm'] == 0]\n",
    "y_group_diammm_1 = simplified_y_train[simplified_x_train['diam_mm'] == 1]\n",
    "\n",
    "y_group_diammm_0_output_label = y_group_diammm_0.value_counts().index[0][0]\n",
    "y_group_diammm_1_output_label = y_group_diammm_1.value_counts().index[0][0]\n",
    "\n",
    "y_group_diammm_0_correct = np.sum(y_group_diammm_0 == y_group_diammm_0_output_label)\n",
    "y_group_diammm_1_correct = np.sum(y_group_diammm_1 == y_group_diammm_1_output_label)\n",
    "\n",
    "correct_rate_diammm = (y_group_diammm_0_correct + y_group_diammm_1_correct) / num_data\n",
    "# couting_based heuristic for height_mm\n",
    "\n",
    "y_group_heightmm_0 = simplified_y_train[simplified_x_train['height_mm'] == 0]\n",
    "y_group_heightmm_1 = simplified_y_train[simplified_x_train['height_mm'] == 1]\n",
    "\n",
    "y_group_heightmm_0_output_label = y_group_heightmm_0.value_counts().index[0][0]\n",
    "y_group_heightmm_1_output_label = y_group_heightmm_1.value_counts().index[0][0]\n",
    "\n",
    "y_group_heightmm_0_correct = np.sum(y_group_heightmm_0 == y_group_heightmm_0_output_label)\n",
    "y_group_heightmm_1_correct = np.sum(y_group_heightmm_1 == y_group_heightmm_1_output_label)\n",
    "\n",
    "correct_rate_heightmm = (y_group_heightmm_0_correct + y_group_heightmm_1_correct) / num_data\n",
    "\n",
    "# print res\n",
    "res_q2 = {}\n",
    "res_q2['is_male'] = correct_rate_ismale.to_numpy()[0]\n",
    "res_q2['length_mm'] = correct_rate_lengthmm.to_numpy()[0]\n",
    "res_q2['diam_mm'] = correct_rate_diammm.to_numpy()[0]\n",
    "res_q2['height_mm'] = correct_rate_heightmm.to_numpy()[0]\n",
    "\n",
    "# print(res_q2)\n",
    "# sorted the result from best to worst based on accurate value\n",
    "res_q2 = sorted(res_q2.items(), key=lambda x: x[1], reverse=True)   \n",
    "# print out the result\n",
    "for f_name, f_value in res_q2:\n",
    "    print(f'{f_name}: {f_value}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (b) Compute the information-theoretic heuristic, and order the features by it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "height_mm: 0.227\n",
      "diam_mm: 0.205\n",
      "length_mm: 0.191\n",
      "is_male: 0.083\n"
     ]
    }
   ],
   "source": [
    "# H(Example)\n",
    "example_num_y0 = np.sum(simplified_y_train==0)\n",
    "example_num_y1 = np.sum(simplified_y_train==1)\n",
    "example_num_y2= np.sum(simplified_y_train==2)\n",
    "H_Example = - (example_num_y0 / num_data * math.log(example_num_y0 / num_data, 2) + example_num_y1 / num_data * math.log(example_num_y1 / num_data, 2) + example_num_y2 / num_data * math.log(example_num_y2 / num_data, 2))\n",
    "\n",
    "# information_based heuristic for is_male\n",
    "num_y0_y_group_ismale_0 = np.sum( y_group_ismale_0 ==0 )\n",
    "num_y1_y_group_ismale_0 = np.sum( y_group_ismale_0 ==1 )\n",
    "H_ismale_0 = -(num_y0_y_group_ismale_0/len(y_group_ismale_0) * math.log(num_y0_y_group_ismale_0/len(y_group_ismale_0) , 2) + num_y1_y_group_ismale_0 / len(y_group_ismale_0) * math.log(num_y1_y_group_ismale_0 / len(y_group_ismale_0), 2))\n",
    "\n",
    "num_y0_y_group_ismale_1 = np.sum( y_group_ismale_1 ==0 )\n",
    "num_y1_y_group_ismale_1 = np.sum( y_group_ismale_1 ==1 )               \n",
    "H_ismale_1 = -(num_y0_y_group_ismale_1/len(y_group_ismale_1) * math.log(num_y0_y_group_ismale_1/len(y_group_ismale_1) , 2) + num_y1_y_group_ismale_1 / len(y_group_ismale_1) * math.log(num_y1_y_group_ismale_1 / len(y_group_ismale_1), 2))\n",
    "\n",
    "H_ismale = len(y_group_ismale_0) / num_data * H_ismale_0 + len(y_group_ismale_1) / num_data * H_ismale_1\n",
    "Gain_ismale = H_Example - H_ismale\n",
    "\n",
    "# information_based heuristic for length_mm\n",
    "num_y0_y_group_lengthmm_0 = np.sum( y_group_lengthmm_0 ==0 )\n",
    "num_y1_y_group_lengthmm_0 = np.sum( y_group_lengthmm_0 ==1 )\n",
    "H_lengthmm_0 = -(num_y0_y_group_lengthmm_0/len(y_group_lengthmm_0) * math.log(num_y0_y_group_lengthmm_0/len(y_group_lengthmm_0) , 2) + num_y1_y_group_lengthmm_0 / len(y_group_lengthmm_0) * math.log(num_y1_y_group_lengthmm_0 / len(y_group_lengthmm_0), 2))\n",
    "\n",
    "num_y0_y_group_lengthmm_1 = np.sum( y_group_lengthmm_1 ==0 )\n",
    "num_y1_y_group_lengthmm_1 = np.sum( y_group_lengthmm_1 ==1 )               \n",
    "H_lengthmm_1 = -(num_y0_y_group_lengthmm_1/len(y_group_lengthmm_1) * math.log(num_y0_y_group_lengthmm_1/len(y_group_lengthmm_1) , 2) + num_y1_y_group_lengthmm_1 / len(y_group_lengthmm_1) * math.log(num_y1_y_group_lengthmm_1 / len(y_group_lengthmm_1), 2))\n",
    "\n",
    "H_lengthmm = len(y_group_lengthmm_0) / num_data * H_lengthmm_0 + len(y_group_lengthmm_1) / num_data * H_lengthmm_1\n",
    "Gain_lengthmm = H_Example - H_lengthmm\n",
    "\n",
    "# couting_based heuristic for diam_mm\n",
    "num_y0_y_group_diammm_0 = np.sum( y_group_diammm_0 ==0 )\n",
    "num_y1_y_group_diammm_0 = np.sum( y_group_diammm_0 ==1 )\n",
    "H_diammm_0 = -(num_y0_y_group_diammm_0/len(y_group_diammm_0) * math.log(num_y0_y_group_diammm_0/len(y_group_diammm_0) , 2) + num_y1_y_group_diammm_0 / len(y_group_diammm_0) * math.log(num_y1_y_group_diammm_0 / len(y_group_diammm_0), 2))\n",
    "\n",
    "num_y0_y_group_diammm_1 = np.sum( y_group_diammm_1 ==0 )\n",
    "num_y1_y_group_diammm_1 = np.sum( y_group_diammm_1 ==1 )               \n",
    "H_diammm_1 = -(num_y0_y_group_diammm_1/len(y_group_diammm_1) * math.log(num_y0_y_group_diammm_1/len(y_group_diammm_1) , 2) + num_y1_y_group_diammm_1 / len(y_group_diammm_1) * math.log(num_y1_y_group_diammm_1 / len(y_group_diammm_1), 2))\n",
    "\n",
    "H_diammm = len(y_group_diammm_0) / num_data * H_diammm_0 + len(y_group_diammm_1) / num_data * H_diammm_1\n",
    "Gain_diammm = H_Example - H_diammm\n",
    "\n",
    "# information_based heuristic for height_mm\n",
    "num_y0_y_group_heightmm_0 = np.sum( y_group_heightmm_0 ==0 )\n",
    "num_y1_y_group_heightmm_0 = np.sum( y_group_heightmm_0 ==1 )\n",
    "H_heightmm_0 = -(num_y0_y_group_heightmm_0/len(y_group_heightmm_0) * math.log(num_y0_y_group_heightmm_0/len(y_group_heightmm_0) , 2) + num_y1_y_group_heightmm_0 / len(y_group_heightmm_0) * math.log(num_y1_y_group_heightmm_0 / len(y_group_heightmm_0), 2))\n",
    "\n",
    "num_y0_y_group_heightmm_1 = np.sum( y_group_heightmm_1 ==0 )\n",
    "num_y1_y_group_heightmm_1 = np.sum( y_group_heightmm_1 ==1 )               \n",
    "H_heightmm_1 = -(num_y0_y_group_heightmm_1/len(y_group_heightmm_1) * math.log(num_y0_y_group_heightmm_1/len(y_group_heightmm_1) , 2) + num_y1_y_group_heightmm_1 / len(y_group_heightmm_1) * math.log(num_y1_y_group_heightmm_1 / len(y_group_heightmm_1), 2))\n",
    "\n",
    "H_heightmm = len(y_group_heightmm_0) / num_data * H_heightmm_0 + len(y_group_heightmm_1) / num_data * H_heightmm_1\n",
    "Gain_heightmm = H_Example - H_heightmm\n",
    "\n",
    "res_q2b = {}\n",
    "res_q2b['is_male'] = Gain_ismale.to_numpy()[0]\n",
    "res_q2b['length_mm'] = Gain_lengthmm.to_numpy()[0]\n",
    "res_q2b['diam_mm'] = Gain_diammm.to_numpy()[0]\n",
    "res_q2b['height_mm'] = Gain_heightmm.to_numpy()[0]\n",
    "\n",
    "# sorted the result from best to worst based on accurate value\n",
    "res_q2b = sorted(res_q2b.items(), key=lambda x: x[1], reverse=True)   \n",
    "# print out the result\n",
    "for f_name, f_value in res_q2b:\n",
    "    print(f'{f_name}: {f_value:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3 Generate decision trees for full- and restricted-feature data\n",
    "\n",
    "#### (a) Print accuracy values and generate tree images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "simplified version - train accuracy:  0.7326826196473551\n",
      "simplified version - test accuracy:  0.722\n",
      "full version - train accuracy:  1.0\n",
      "full version - test accuracy:  0.196\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'full_tree.pdf'"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import tree\n",
    "# simplified data-set\n",
    "simp_x_train = pd.read_csv(\"data_abalone/small_binary_x_train.csv\")\n",
    "simp_y_train = pd.read_csv(\"data_abalone/3class_y_train.csv\")\n",
    "\n",
    "simp_x_test = pd.read_csv(\"data_abalone/small_binary_x_test.csv\")\n",
    "simp_y_test = pd.read_csv(\"data_abalone/3class_y_test.csv\")\n",
    "\n",
    "clf = tree.DecisionTreeClassifier()\n",
    "clf.fit(simp_x_train, simp_y_train)\n",
    "simp_accurate_train = clf.score(simp_x_train, simp_y_train)\n",
    "simp_accurate_test = clf.score(simp_x_test, simp_y_test)\n",
    "\n",
    "print(\"simplified version - train accuracy: \", simp_accurate_train)\n",
    "print(\"simplified version - test accuracy: \", simp_accurate_test)\n",
    "\n",
    "dot_data = tree.export_graphviz(clf, out_file=None, feature_names=list(simp_x_train.columns))\n",
    "graph = graphviz.Source(dot_data)\n",
    "graph.render(\"simp_tree\")\n",
    "# full version\n",
    "full_x_train = pd.read_csv(\"data_abalone/x_train.csv\")\n",
    "full_y_train = pd.read_csv(\"data_abalone/y_train.csv\")\n",
    "\n",
    "full_x_test = pd.read_csv(\"data_abalone/x_test.csv\")\n",
    "full_y_test = pd.read_csv(\"data_abalone/y_test.csv\")\n",
    "\n",
    "clf2 = tree.DecisionTreeClassifier()\n",
    "clf2.fit(full_x_train, full_y_train)\n",
    "full_accurate_train = clf2.score(full_x_train, full_y_train)\n",
    "full_accurate_test = clf2.score(full_x_test, full_y_test)\n",
    "\n",
    "print(\"full version - train accuracy: \",full_accurate_train)\n",
    "print(\"full version - test accuracy: \",full_accurate_test)\n",
    "\n",
    "dot_data2 = tree.export_graphviz(clf2, out_file=None, feature_names=list(full_x_train.columns))\n",
    "graph2 = graphviz.Source(dot_data2)\n",
    "graph2.render(\"full_tree\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (b) Discuss the results seen for the two trees\n",
    "\n",
    "- Discuss the results you have just seen. \n",
    "    > The simplified version tree is relatively small and balanced, while the full version tree is huge and relatively unbalanced\n",
    "- What do the various accuracy-score values tell you? \n",
    "    > The the accuracy-score of the simplified version tells us that this model performs normal (i.e. training accuracy score is slightly better than testing score), while the accurarcy-score of the full version tells us that this model overfits the data-set, since it fits the training set perfect but screws up on the testing set. \n",
    "- How do the two trees that are produced differ? \n",
    "    > The depth of the simplified version tree is small and well-balanced, and at most of leaves, the gini value is not zero ( i.e. all samples in that group don't have the same output). While the full version is really huge, although most of the leaves have the gini value zero, but also most of the leaves usually only include just one sample.\n",
    "- Looking at the outputs (leaves) of the simplified-data tree, what sorts of errors does that tree make?\n",
    "    > The errors it makes is that almost all of its leaves are not pure (i.e. gini value is not zero, or all samples in the leaf do not in the same class). Because ideally we expect all samples in the leaf would end up being in the same output class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
